[package]
name = "ollama-lmstudio-proxy"
version = "0.2.0"
edition = "2024"
authors = ["uwuclxdy"]
description = "High-performance proxy server that bridges Ollama API and LM Studio"
license = "MIT"
repository = "https://github.com/uwuclxdy/ollama-lmstudio-proxy"
keywords = ["ollama", "lmstudio", "proxy", "llm", "ai"]

[dependencies]
tokio = { version = "1.48.0", features = ["full"] }
tokio-util = "0.7.17"
tokio-stream = "0.1.17"
warp = { version = "0.4.2", features = ["server"] }
http-body-util = "0.1.3"
reqwest = { version = "0.12.26", features = ["json", "stream"] }
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"
futures-util = "0.3.31"
bytes = "1.11.0"
chrono = { version = "0.4.42", features = ["serde"] }
clap = { version = "4.5.53", features = ["derive"] }
log = "0.4.29"
fern = "0.7.1"
moka = { version = "0.12.11", features = ["future"] }
sha2 = "0.10.9"
md5 = "0.8.0"
url = "2.5.7"
humantime = "2.3.0"

[profile.release]
lto = true
codegen-units = 1
opt-level = 3
strip = true

[profile.dev]
opt-level = 1
