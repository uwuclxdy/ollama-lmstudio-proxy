#[cfg(test)]
#[allow(clippy::module_inception)]
mod ollama_endpoints_tests {
    use crate::constants::OLLAMA_SERVER_VERSION;
    use serde_json::json;

    /// Test /api/tags response structure according to Ollama API docs
    #[test]
    fn test_ollama_tags_response_structure() {
        // According to ollama.md: GET /api/tags should return models array with specific fields
        let response = json!({
            "models": [
                {
                    "name": "deepseek-r1:latest",
                    "model": "deepseek-r1:latest",
                    "modified_at": "2025-05-10T08:06:48.639712648-07:00",
                    "size": 4683075271_u64,
                    "digest": "0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163",
                    "details": {
                        "parent_model": "",
                        "format": "gguf",
                        "family": "qwen2",
                        "families": ["qwen2"],
                        "parameter_size": "7.6B",
                        "quantization_level": "Q4_K_M"
                    }
                }
            ]
        });

        // Verify structure
        assert!(response.get("models").is_some());
        let models = response["models"].as_array().unwrap();
        assert!(!models.is_empty());

        let model = &models[0];
        assert!(model.get("name").is_some());
        assert!(model.get("model").is_some());
        assert!(model.get("modified_at").is_some());
        assert!(model.get("size").is_some());
        assert!(model.get("digest").is_some());
        assert!(model.get("details").is_some());

        let details = &model["details"];
        assert!(details.get("format").is_some());
        assert!(details.get("family").is_some());
        assert!(details.get("families").is_some());
        assert!(details.get("parameter_size").is_some());
        assert!(details.get("quantization_level").is_some());
    }

    /// Test /api/ps response structure for running models
    #[test]
    fn test_ollama_ps_response_structure() {
        // According to ollama.md: GET /api/ps lists running models with additional fields
        let response = json!({
            "models": [
                {
                    "name": "mistral:latest",
                    "model": "mistral:latest",
                    "size": 5137025024_u64,
                    "digest": "2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8",
                    "details": {
                        "parent_model": "",
                        "format": "gguf",
                        "family": "llama",
                        "families": ["llama"],
                        "parameter_size": "7.2B",
                        "quantization_level": "Q4_0"
                    },
                    "expires_at": "2024-06-04T14:38:31.83753-07:00",
                    "size_vram": 5137025024_u64
                }
            ]
        });

        // Verify ps-specific fields
        let model = &response["models"][0];
        assert!(model.get("expires_at").is_some());
        assert!(model.get("size_vram").is_some());
    }

    /// Test /api/show response structure
    #[test]
    fn test_ollama_show_response_structure() {
        // According to ollama.md: POST /api/show returns detailed model info
        let response = json!({
            "modelfile": "# Modelfile generated by \"ollama show\"",
            "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"",
            "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "llama",
                "families": ["llama"],
                "parameter_size": "8.0B",
                "quantization_level": "Q4_0"
            },
            "model_info": {
                "general.architecture": "llama",
                "general.file_type": 2,
                "general.parameter_count": 8030261248_u64,
                "llama.attention.head_count": 32,
                "llama.context_length": 8192
            },
            "capabilities": ["completion", "vision"]
        });

        assert!(response.get("modelfile").is_some());
        assert!(response.get("parameters").is_some());
        assert!(response.get("template").is_some());
        assert!(response.get("details").is_some());
        assert!(response.get("model_info").is_some());
        assert!(response.get("capabilities").is_some());

        let capabilities = response["capabilities"].as_array().unwrap();
        assert!(!capabilities.is_empty());
    }

    /// Test /api/chat streaming response format
    #[test]
    fn test_ollama_chat_streaming_response() {
        // According to ollama.md: streaming chat returns series of JSON objects
        let chunk = json!({
            "model": "llama3.2",
            "created_at": "2023-08-04T08:52:19.385406455-07:00",
            "message": {
                "role": "assistant",
                "content": "The"
            },
            "done": false
        });

        assert_eq!(chunk["model"], "llama3.2");
        assert_eq!(chunk["done"], false);
        assert!(chunk["message"].get("role").is_some());
        assert!(chunk["message"].get("content").is_some());

        // Final chunk with timing
        let final_chunk = json!({
            "model": "llama3.2",
            "created_at": "2023-08-04T19:22:45.499127Z",
            "message": {
                "role": "assistant",
                "content": ""
            },
            "done": true,
            "total_duration": 4883583458_u64,
            "load_duration": 1334875_u64,
            "prompt_eval_count": 26,
            "prompt_eval_duration": 342546000_u64,
            "eval_count": 282,
            "eval_duration": 4535599000_u64
        });

        assert_eq!(final_chunk["done"], true);
        assert!(final_chunk.get("total_duration").is_some());
        assert!(final_chunk.get("load_duration").is_some());
        assert!(final_chunk.get("prompt_eval_count").is_some());
        assert!(final_chunk.get("eval_count").is_some());
    }

    /// Test /api/chat non-streaming response
    #[test]
    fn test_ollama_chat_non_streaming_response() {
        // According to ollama.md: non-streaming returns complete response
        let response = json!({
            "model": "llama3.2",
            "created_at": "2023-12-12T14:13:43.416799Z",
            "message": {
                "role": "assistant",
                "content": "Hello! How are you today?"
            },
            "done": true,
            "total_duration": 5191566416_u64,
            "load_duration": 2154458_u64,
            "prompt_eval_count": 26,
            "prompt_eval_duration": 383809000_u64,
            "eval_count": 298,
            "eval_duration": 4799921000_u64
        });

        assert_eq!(response["done"], true);
        assert!(!response["message"]["content"].as_str().unwrap().is_empty());
        assert!(response["total_duration"].as_u64().unwrap() > 0);
    }

    /// Test /api/chat with tools response
    #[test]
    fn test_ollama_chat_tool_calls_response() {
        // According to ollama.md: tool calls appear in message.tool_calls
        let response = json!({
            "model": "llama3.2",
            "created_at": "2025-07-07T20:32:53.844124Z",
            "message": {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "function": {
                            "name": "get_weather",
                            "arguments": {
                                "city": "Tokyo"
                            }
                        }
                    }
                ]
            },
            "done_reason": "stop",
            "done": true,
            "total_duration": 3244883583_u64
        });

        let tool_calls = response["message"]["tool_calls"].as_array().unwrap();
        assert!(!tool_calls.is_empty());

        let tool_call = &tool_calls[0];
        assert_eq!(tool_call["function"]["name"], "get_weather");
        assert!(tool_call["function"]["arguments"].get("city").is_some());
    }

    /// Test /api/generate streaming response
    #[test]
    fn test_ollama_generate_streaming_response() {
        // According to ollama.md: streaming generate returns response field
        let chunk = json!({
            "model": "llama3.2",
            "created_at": "2023-08-04T08:52:19.385406455-07:00",
            "response": "The",
            "done": false
        });

        assert_eq!(chunk["response"], "The");
        assert_eq!(chunk["done"], false);

        // Final chunk
        let final_chunk = json!({
            "model": "llama3.2",
            "created_at": "2023-08-04T19:22:45.499127Z",
            "response": "",
            "done": true,
            "total_duration": 10706818083_u64,
            "load_duration": 6338219291_u64,
            "prompt_eval_count": 26,
            "prompt_eval_duration": 130079000_u64,
            "eval_count": 259,
            "eval_duration": 4232710000_u64
        });

        assert_eq!(final_chunk["done"], true);
        assert!(final_chunk.get("total_duration").is_some());
    }

    /// Test /api/generate with structured output request
    #[test]
    fn test_ollama_generate_structured_output_request() {
        // According to ollama.md: format can be JSON or JSON schema
        let request = json!({
            "model": "llama3.1:8b",
            "prompt": "Ollama is 22 years old. Respond using JSON",
            "stream": false,
            "format": {
                "type": "object",
                "properties": {
                    "age": { "type": "integer" },
                    "available": { "type": "boolean" }
                },
                "required": ["age", "available"]
            }
        });

        assert!(request.get("format").is_some());
        let format = &request["format"];
        assert_eq!(format["type"], "object");
        assert!(format.get("properties").is_some());
        assert!(format.get("required").is_some());
    }

    /// Test /api/generate with JSON mode
    #[test]
    fn test_ollama_generate_json_mode_request() {
        // According to ollama.md: format can be "json" string
        let request = json!({
            "model": "llama3.2",
            "prompt": "What color is the sky? Respond using JSON",
            "format": "json",
            "stream": false
        });

        assert_eq!(request["format"], "json");
    }

    /// Test /api/generate advanced parameters
    #[test]
    fn test_ollama_generate_advanced_parameters() {
        // According to ollama.md: suffix/system/template/raw/think/keep_alive/context are supported
        let request = json!({
            "model": "llama3.2",
            "prompt": "Why is the sky blue?",
            "suffix": " Thanks!",
            "system": "You are concise.",
            "template": "User: {{prompt}}",
            "raw": true,
            "think": true,
            "keep_alive": "5m",
            "context": [1, 2, 3],
            "stream": false
        });

        assert_eq!(request["suffix"], " Thanks!");
        assert_eq!(request["system"], "You are concise.");
        assert_eq!(request["template"], "User: {{prompt}}");
        assert_eq!(request["raw"], true);
        assert_eq!(request["think"], true);
        assert_eq!(request["keep_alive"], "5m");
        assert!(request["context"].is_array());
    }

    /// Test /api/embeddings request structure
    #[test]
    fn test_ollama_embeddings_request() {
        // According to ollama.md: embeddings accept input or prompt
        let request = json!({
            "model": "all-minilm",
            "input": "Why is the sky blue?"
        });

        assert!(request.get("input").is_some());
    }

    /// Test /api/embeddings response structure
    #[test]
    fn test_ollama_embeddings_response() {
        // According to ollama.md: returns embeddings array
        let response = json!({
            "model": "all-minilm",
            "embeddings": [
                [0.010071029, -0.0017594862, 0.05007221, 0.04692972]
            ],
            "total_duration": 14143917_u64,
            "load_duration": 1019500_u64,
            "prompt_eval_count": 8
        });

        let embeddings = response["embeddings"].as_array().unwrap();
        assert!(!embeddings.is_empty());
        let embedding = embeddings[0].as_array().unwrap();
        assert!(!embedding.is_empty());

        assert!(response.get("total_duration").is_some());
        assert!(response.get("load_duration").is_some());
    }

    /// Test /api/embeddings with multiple inputs
    #[test]
    fn test_ollama_embeddings_multiple_inputs() {
        // According to ollama.md: input can be array
        let request = json!({
            "model": "all-minilm",
            "input": ["Why is the sky blue?", "Why is the grass green?"]
        });

        let inputs = request["input"].as_array().unwrap();
        assert_eq!(inputs.len(), 2);
    }

    /// Test /api/embed documented advanced parameters
    #[test]
    fn test_ollama_embeddings_parameters() {
        let request = json!({
            "model": "all-minilm",
            "input": "text",
            "truncate": false,
            "keep_alive": "10m",
            "dimensions": 128
        });

        assert_eq!(request["truncate"], false);
        assert_eq!(request["dimensions"], 128);
        assert_eq!(request["keep_alive"], "10m");
    }

    /// Test /api/version response
    #[test]
    fn test_ollama_version_response() {
        // According to ollama.md: version returns version string
        let response = json!({
            "version": OLLAMA_SERVER_VERSION
        });

        assert!(response.get("version").is_some());
        assert!(response["version"].is_string());
    }

    /// Test load model hint (empty messages)
    #[test]
    fn test_ollama_chat_load_hint() {
        // According to ollama.md: empty messages array triggers load
        let request = json!({
            "model": "llama3.2",
            "messages": []
        });

        assert_eq!(request["messages"].as_array().unwrap().len(), 0);

        // Response should indicate load
        let response = json!({
            "model": "llama3.2",
            "created_at": "2024-09-12T21:17:29.110811Z",
            "message": {
                "role": "assistant",
                "content": ""
            },
            "done_reason": "load",
            "done": true
        });

        assert_eq!(response["done_reason"], "load");
    }

    /// Test unload model hint (keep_alive=0)
    #[test]
    fn test_ollama_chat_unload_hint() {
        // According to ollama.md: empty messages with keep_alive=0 triggers unload
        let request = json!({
            "model": "llama3.2",
            "messages": [],
            "keep_alive": 0
        });

        assert_eq!(request["keep_alive"], 0);

        let response = json!({
            "model": "llama3.2",
            "created_at": "2024-09-12T21:33:17.547535Z",
            "message": {
                "role": "assistant",
                "content": ""
            },
            "done_reason": "unload",
            "done": true
        });

        assert_eq!(response["done_reason"], "unload");
    }

    /// Test options parameter mappings
    #[test]
    fn test_ollama_options_parameters() {
        // According to ollama.md: options supports various parameters
        let request = json!({
            "model": "llama3.2",
            "prompt": "Test",
            "options": {
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 20,
                "repeat_penalty": 1.2,
                "presence_penalty": 1.5,
                "frequency_penalty": 1.0,
                "seed": 42,
                "num_predict": 100,
                "stop": ["\n", "user:"]
            }
        });

        let options = &request["options"];
        assert!(options.get("temperature").is_some());
        assert!(options.get("top_p").is_some());
        assert!(options.get("top_k").is_some());
        assert!(options.get("repeat_penalty").is_some());
        assert!(options.get("seed").is_some());
        assert!(options.get("num_predict").is_some());
    }

    /// Test logit_bias parameter
    #[test]
    fn test_ollama_logit_bias() {
        // According to README: logit_bias to discourage/force tokens
        let request = json!({
            "model": "llama3.1:8b",
            "messages": [{"role": "user", "content": "Reply yes or no"}],
            "options": {
                "logit_bias": {
                    "464": -10,   // discourage "No"
                    "302": 15     // prefer "Yes"
                }
            }
        });

        let logit_bias = &request["options"]["logit_bias"];
        assert!(logit_bias.is_object());
        assert_eq!(logit_bias["464"], -10);
        assert_eq!(logit_bias["302"], 15);
    }

    /// Test /api/pull streaming status chunks
    #[test]
    fn test_ollama_pull_streaming_status() {
        let initial = json!({
            "status": "pulling manifest",
            "model": "llama3.2",
            "detail": "queued"
        });

        let success = json!({
            "status": "success",
            "model": "llama3.2",
            "total": 1024_u64,
            "completed": 1024_u64
        });

        assert_eq!(initial["status"], "pulling manifest");
        assert_eq!(success["status"], "success");
        assert!(success.get("total").is_some());
        assert!(success.get("completed").is_some());
    }

    /// Test /api/create streamed status objects
    #[test]
    fn test_ollama_create_streaming_status() {
        let chunks = [
            json!({"status": "reading model metadata"}),
            json!({
                "status": "quantizing F16 model to Q4_K_M",
                "digest": "0",
                "total": 6433687776_u64,
                "completed": 12302_u64
            }),
            json!({"status": "writing manifest"}),
            json!({"status": "success"}),
        ];

        assert_eq!(chunks.first().unwrap()["status"], "reading model metadata");
        assert_eq!(chunks.last().unwrap()["status"], "success");
        assert!(chunks[1].get("digest").is_some());
        assert!(chunks[1].get("total").is_some());
    }

    /// Test /api/create alias response shape
    #[test]
    fn test_ollama_create_alias_response() {
        let response = json!({
            "status": "success",
            "model": "chatbot",
            "virtual": true
        });

        assert_eq!(response["status"], "success");
        assert_eq!(response["virtual"], true);
        assert!(response.get("model").is_some());
    }

    /// Test /api/copy response shape
    #[test]
    fn test_ollama_copy_response() {
        let response = json!({
            "status": "success",
            "model": "chatbot-copy"
        });

        assert_eq!(response["model"], "chatbot-copy");
        assert!(response.get("status").is_some());
    }

    /// Test /api/delete response shape
    #[test]
    fn test_ollama_delete_response() {
        let response = json!({
            "status": "success",
            "model": "chatbot"
        });

        assert_eq!(response["status"], "success");
        assert!(response.get("model").is_some());
    }

    /// Test /api/push streaming status responses
    #[test]
    fn test_ollama_push_streaming_status() {
        let chunks = [
            json!({"status": "retrieving manifest", "model": "llama3.2"}),
            json!({"status": "starting upload", "model": "llama3.2"}),
            json!({"status": "success", "model": "llama3.2"}),
        ];

        assert_eq!(chunks.first().unwrap()["status"], "retrieving manifest");
        assert_eq!(chunks.last().unwrap()["status"], "success");
    }

    /// Test /api/blobs HEAD/POST status codes
    #[test]
    fn test_ollama_blob_status_codes() {
        let exists_status = 200_u16;
        let missing_status = 404_u16;
        let upload_status = 201_u16;

        assert_eq!(exists_status, warp::http::StatusCode::OK.as_u16());
        assert_eq!(missing_status, warp::http::StatusCode::NOT_FOUND.as_u16());
        assert_eq!(upload_status, warp::http::StatusCode::CREATED.as_u16());
    }
}
